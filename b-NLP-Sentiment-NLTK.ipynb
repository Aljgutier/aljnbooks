{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sentiment with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load  data ...\n",
      "[0 0 0 ... 2 2 2]\n",
      "loaded data: 75000 25000\n",
      "shuffled data: 75000 75000\n",
      "labled data: 25000 25000\n",
      "reviews clean: 25000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Pre-process: Shuffle, Keep Labeled Data, Clean\n",
    "print(\"load  data ...\")\n",
    "train_texts,train_labels = get_texts(PATH/'train')   # extrating training and test from here\n",
    "val_texts,val_labels = get_texts(PATH/'test')        # holdout set\n",
    "print(train_labels)\n",
    "print(\"loaded data:\" , len(train_texts),len(val_texts))\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.permutation(len(train_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "train_texts = train_texts[train_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "train_labels = train_labels[train_idx]\n",
    "val_labels = val_labels[val_idx]\n",
    "print(\"shuffled data:\", len(train_texts), len(train_labels))\n",
    "\n",
    "# Keep Labeled Data\n",
    "idx=np.where(train_labels != 2 )[0]\n",
    "train_texts = train_texts[idx]\n",
    "train_labels = train_labels[idx]\n",
    "print(\"labled data:\" , len(train_texts), len(train_labels))\n",
    "\n",
    "# Clean\n",
    "#  run the htmlfix and pre-process functions (appendix)\n",
    "reviews_train_clean = preprocess_reviews(train_texts) # train ... extract training and validation from this\n",
    "reviews_val_clean = preprocess_reviews(val_texts)  # test holdout set\n",
    "print(\"reviews clean:\", len(reviews_train_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize ...\n",
      "lematized: 25000 25000\n",
      "CountvVectorizor ... \n",
      "vecctorized: (25000, 5443695)\n"
     ]
    }
   ],
   "source": [
    "# Step 2:  Tokenize and Vectorize\n",
    "print(\"CountvVectorizor ... \")\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "# change the two inputs below to be reviews_train_clean_lem ... if you want to use lemmatized text\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_val = ngram_vectorizer.transform(reviews_val_clean)\n",
    "print(\"vecctorized:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.90024\n"
     ]
    }
   ],
   "source": [
    "# Step 3 Sentiment Classification\n",
    "final = LinearSVC(C=0.01)\n",
    "final.fit(X, train_labels)\n",
    "print (\"Final Accuracy: %s\"  % accuracy_score(val_labels, final.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract ... Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from path import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the future can directly read the csv files for convenience ... \n",
    "# Load np arrays directly from files\n",
    "#   an easier way might be to load a dataframe and then convert to np.array\n",
    "#   but the information is in separate files, so would need create a dataframe\n",
    "#   USF Fastai does it this way, and then creates a DataFrame\n",
    "\n",
    "PATH=Path('./data/aclImdb')\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# htmlfix and pre-process functions\n",
    "\n",
    "# htmlfix function\n",
    "re1 = re.compile(r'  +')\n",
    "def htmlfix(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "punctuationfix = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "# Remove punctuation\n",
    "# Remove html \n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [punctuationfix.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews= [ htmlfix(line) for line in reviews]\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Wordnet Lemmatizer\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
